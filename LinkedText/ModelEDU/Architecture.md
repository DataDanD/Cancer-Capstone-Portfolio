# Algorithm Design

## Trees

## Radom Forest

Radom forest has a bunch of decision tree with randomly selected features to split on, 
and randomly selected data to train each unique tree. 
All of the trees in the forest are used in the prediction process.
Taking the mode of all tree predictions for a classification model or 
the mean value from all the tree predictions in a regression model.

## Boosting Overview

### Gradient Boosting

### Adaboost

### Catboost


### XGBoost


## Sources

http://dataaspirant.com/2017/05/22/random-forest-algorithm-machine-learing/
http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/
[class vs regress](http://www.simafore.com/blog/bid/62482/2-main-differences-between-classification-and-regression-trees)
